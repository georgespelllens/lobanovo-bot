"""
Load Telegram channel exports into the knowledge base.

Supports:
- HTML (Telegram Desktop export) ‚Äî –∫–∞–Ω–∞–ª ¬´–õ–æ–±–∞–Ω–æ–≤–æ –ù–∞—Å—Ç–∞–≤–Ω–∏—á–µ—Å—Ç–≤–æ¬ª
- MD (text export) ‚Äî –∫–∞–Ω–∞–ª ¬´–ë–æ—Ä–æ–¥–∞—Ç—ã–π, –ª—ã—Å—ã–π, —Ç–≤–æ–π¬ª

Pipeline:
  1. load_knowledge_base.py --all --dry-run   # –ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É (–±–µ–∑ –∑–∞–ø–∏—Å–∏ –≤ –ë–î)
  2. load_knowledge_base.py --all             # –ó–∞–≥—Ä—É–∑–∏—Ç—å —Å –ø—Ä–µ–¥—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π
  3. filter_quality.py                        # LLM-–æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ ($2-3)
  4. generate_embeddings.py                   # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞

Usage:
  python scripts/load_knowledge_base.py --all --dry-run
  python scripts/load_knowledge_base.py --all
  python scripts/load_knowledge_base.py --file data/–ª–æ–±–∞–Ω–æ–≤–æ.html --source nastavnichestvo_channel --format html
  python scripts/load_knowledge_base.py --file data/–±–æ—Ä–æ–¥–∞—Ç1.md --source main_channel --format md
"""

import argparse
import asyncio
import hashlib
import re
import sys
import os
from html.parser import HTMLParser
from datetime import datetime

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# DB imports are deferred ‚Äî not needed for --dry-run
_db_imported = False
get_session = None
KnowledgeBase = None


def _ensure_db_imports():
    """Lazy-import DB modules (not needed for --dry-run)."""
    global _db_imported, get_session, KnowledgeBase
    if not _db_imported:
        from src.database.connection import get_session as _gs
        from src.database.models import KnowledgeBase as _kb
        get_session = _gs
        KnowledgeBase = _kb
        _db_imported = True


# ‚îÄ‚îÄ‚îÄ Content filters (pre-LLM, rule-based) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

MIN_CONTENT_LENGTH = 200  # –ú–∏–Ω–∏–º—É–º —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –ø–æ–ª–µ–∑–Ω–æ–≥–æ –ø–æ—Å—Ç–∞

# –ü–∞—Ç—Ç–µ—Ä–Ω—ã –º—É—Å–æ—Ä–∞ ‚Äî –ø–æ—Å—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ —Ç–æ—á–Ω–æ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –ø–æ–ª–µ–∑–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
JUNK_PATTERNS = [
    # –†–µ–ø–æ—Å—Ç—ã –∏ —Å—Å—ã–ª–∫–∏ –±–µ–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    re.compile(r"^https?://\S+$"),                        # –ü—Ä–æ—Å—Ç–æ —Å—Å—ã–ª–∫–∞
    re.compile(r"^(Forwarded from|–ü–µ—Ä–µ—Å–ª–∞–Ω–æ –∏–∑)"),         # –†–µ–ø–æ—Å—Ç
    # –ì–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è, —Å—Ç–∏–∫–µ—Ä—ã, –º–µ–¥–∏–∞ –±–µ–∑ —Ç–µ–∫—Å—Ç–∞
    re.compile(r"^(Anonymous poll|Quiz)"),                 # –û–ø—Ä–æ—Å
    re.compile(r"^Photo$|^Video$|^Sticker$"),              # –ú–µ–¥–∏–∞-–∑–∞–≥–ª—É—à–∫–∏
    # –°–ª—É–∂–µ–±–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
    re.compile(r"^(Pinned message|Channel created)"),      # –°–∏—Å—Ç–µ–º–Ω—ã–µ
    re.compile(r"^(joined|left) the (group|channel)"),     # –í—Ö–æ–¥—ã/–≤—ã—Ö–æ–¥—ã
]

# –ï—Å–ª–∏ >60% —Ç–µ–∫—Å—Ç–∞ ‚Äî —Å—Å—ã–ª–∫–∏, —ç—Ç–æ –Ω–µ –∫–æ–Ω—Ç–µ–Ω—Ç
URL_RATIO_THRESHOLD = 0.6


def is_junk_content(text: str) -> str | None:
    """Check if content is junk. Returns reason string or None if OK."""
    # –î–ª–∏–Ω–∞
    if len(text) < MIN_CONTENT_LENGTH:
        return f"too_short ({len(text)} < {MIN_CONTENT_LENGTH})"

    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –º—É—Å–æ—Ä–∞
    for pattern in JUNK_PATTERNS:
        if pattern.search(text):
            return f"junk_pattern ({pattern.pattern[:40]})"

    # –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —Å—Å—ã–ª–æ–∫
    urls = re.findall(r"https?://\S+", text)
    url_chars = sum(len(u) for u in urls)
    if len(text) > 0 and url_chars / len(text) > URL_RATIO_THRESHOLD:
        return f"mostly_links ({url_chars}/{len(text)} = {url_chars/len(text):.0%})"

    # –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —ç–º–æ–¥–∑–∏ / –º–∞–ª–æ –±—É–∫–≤ (–ø–æ–∑–¥—Ä–∞–≤–ª–µ–Ω–∏—è, —Ä–µ–∞–∫—Ü–∏–∏)
    letters = len(re.findall(r"[–∞-—è–ê-–Øa-zA-Z]", text))
    if len(text) > 0 and letters / len(text) < 0.3:
        return f"low_text_ratio ({letters}/{len(text)} = {letters/len(text):.0%})"

    return None


# ‚îÄ‚îÄ‚îÄ HTML Parser (for ¬´–õ–æ–±–∞–Ω–æ–≤–æ –ù–∞—Å—Ç–∞–≤–Ω–∏—á–µ—Å—Ç–≤–æ¬ª) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

class TelegramHTMLParser(HTMLParser):
    """Parser for Telegram Desktop HTML export."""

    def __init__(self):
        super().__init__()
        self.posts = []
        self.current_text = []
        self.current_date = None
        self.in_text = False

    def handle_starttag(self, tag, attrs):
        classes = dict(attrs).get("class", "")
        if "text" in classes and "from_name" not in classes and "date" not in classes:
            self.in_text = True
            self.current_text = []
        if "date details" in classes:
            title = dict(attrs).get("title", "")
            if title:
                self.current_date = title

    def handle_endtag(self, tag):
        if self.in_text and tag == "div":
            text = " ".join(self.current_text).strip()
            text = re.sub(r"\s+", " ", text)
            if text:
                self.posts.append({"content": text, "date": self.current_date})
            self.in_text = False

    def handle_data(self, data):
        if self.in_text:
            self.current_text.append(data.strip())


# ‚îÄ‚îÄ‚îÄ MD Parser (for ¬´–ë–æ—Ä–æ–¥–∞—Ç—ã–π, –ª—ã—Å—ã–π, —Ç–≤–æ–π¬ª) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def parse_md_channel(file_path: str, channel_name: str = None) -> list:
    """Parse MD export of Telegram channel.
    
    Auto-detects channel name from first line if not provided.
    Splits content by channel name marker to extract individual posts.
    """
    with open(file_path, "r", encoding="utf-8") as f:
        content = f.read()

    # Auto-detect channel name: pick the candidate with most occurrences
    if not channel_name:
        candidates = [
            "–ë–æ—Ä–æ–¥–∞—Ç—ã–π, –ª—ã—Å—ã–π, —Ç–≤–æ–π",
            "–õ–æ–±–∞–Ω–æ–≤–æ –ù–∞—Å—Ç–∞–≤–Ω–∏—á–µ—Å—Ç–≤–æ",
        ]
        best_name = None
        best_count = 0
        for c in candidates:
            count = content.count(c)
            if count > best_count:
                best_count = count
                best_name = c
        channel_name = best_name or "–ë–æ—Ä–æ–¥–∞—Ç—ã–π, –ª—ã—Å—ã–π, —Ç–≤–æ–π"

    # Split by channel name marker
    blocks = re.split(re.escape(channel_name), content)
    posts = []

    for block in blocks:
        # Extract date if present
        date_match = re.search(r"(\d{1,2}\.\d{2}\.\d{4})", block)
        date_str = date_match.group(1) if date_match else None

        # Parse date
        date = None
        if date_str:
            try:
                date = datetime.strptime(date_str, "%d.%m.%Y")
            except ValueError:
                pass

        # Clean metadata
        clean = block
        clean = re.sub(r"PhotoNot included.*?KB", "", clean)
        clean = re.sub(r"Video fileNot included.*?MB", "", clean)
        clean = re.sub(r"StickerNot included.*?KB", "", clean)
        clean = re.sub(r"\[Previous messages\]\(.*?\)", "", clean)
        clean = re.sub(r"Anonymous poll.*?votes", "", clean, flags=re.DOTALL)
        # Remove reactions
        clean = re.sub(r"[üî•‚ù§üëçüåöüòÅ‚≠êü´°üíØ‚ù§‚Äçüî•üëãüå≠üçìüëæüí©üóø]\d*", "", clean)
        # Remove markdown bold
        clean = re.sub(r"\*\*", "", clean)
        # Normalize whitespace
        clean = re.sub(r"\s+", " ", clean).strip()

        if clean:
            posts.append({"content": clean, "date": date})

    return posts


# ‚îÄ‚îÄ‚îÄ Parsing dispatcher ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def parse_file(file_path: str, format: str) -> list:
    """Parse a file and return raw posts. Auto-detects channel name for MD."""
    if format == "html":
        parser = TelegramHTMLParser()
        with open(file_path, "r", encoding="utf-8") as f:
            parser.feed(f.read())
        return parser.posts
    elif format == "md":
        return parse_md_channel(file_path)  # Auto-detects channel name
    else:
        raise ValueError(f"Unknown format: {format}")


def filter_posts(posts: list) -> tuple[list, dict]:
    """Apply rule-based filters. Returns (good_posts, stats)."""
    stats = {"total_parsed": len(posts), "accepted": 0, "rejected": {}}
    good = []

    for post in posts:
        reason = is_junk_content(post["content"])
        if reason:
            bucket = reason.split(" ")[0]  # e.g. "too_short"
            stats["rejected"][bucket] = stats["rejected"].get(bucket, 0) + 1
        else:
            good.append(post)
            stats["accepted"] += 1

    return good, stats


# ‚îÄ‚îÄ‚îÄ Dry-run: statistics only ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def print_dry_run_stats(source: str, file_path: str, posts_raw: list, posts_good: list, stats: dict):
    """Print detailed statistics without writing to DB."""
    print(f"\n{'='*60}")
    print(f"üìä {source} ‚Äî {os.path.basename(file_path)}")
    print(f"{'='*60}")
    print(f"  –°–ø–∞—Ä—Å–µ–Ω–æ:       {stats['total_parsed']}")
    print(f"  –ü—Ä–∏–Ω—è—Ç–æ:        {stats['accepted']} ({stats['accepted']/max(stats['total_parsed'],1)*100:.0f}%)")
    print(f"  –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ:  {stats['total_parsed'] - stats['accepted']}")

    if stats["rejected"]:
        print(f"  –ü—Ä–∏—á–∏–Ω—ã –æ—Ç—Å–µ–≤–∞:")
        for reason, count in sorted(stats["rejected"].items(), key=lambda x: -x[1]):
            print(f"    {reason}: {count}")

    if posts_good:
        lengths = [len(p["content"]) for p in posts_good]
        print(f"\n  –î–ª–∏–Ω–∞ –ø–æ—Å—Ç–æ–≤ (–ø—Ä–∏–Ω—è—Ç—ã—Ö):")
        print(f"    –º–∏–Ω:     {min(lengths)} —Å–∏–º–≤.")
        print(f"    –º–µ–¥–∏–∞–Ω–∞: {sorted(lengths)[len(lengths)//2]} —Å–∏–º–≤.")
        print(f"    –º–∞–∫—Å:    {max(lengths)} —Å–∏–º–≤.")
        print(f"    —Å—Ä–µ–¥–Ω—è—è: {sum(lengths)/len(lengths):.0f} —Å–∏–º–≤.")

        # Distribution by length buckets
        buckets = {"200-500": 0, "500-1000": 0, "1000-2000": 0, "2000+": 0}
        for l in lengths:
            if l < 500:
                buckets["200-500"] += 1
            elif l < 1000:
                buckets["500-1000"] += 1
            elif l < 2000:
                buckets["1000-2000"] += 1
            else:
                buckets["2000+"] += 1
        print(f"    —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {buckets}")

        # Date range
        dates = [p["date"] for p in posts_good if p.get("date")]
        if dates:
            date_objs = []
            for d in dates:
                if isinstance(d, datetime):
                    date_objs.append(d)
                elif isinstance(d, str):
                    try:
                        date_objs.append(datetime.strptime(d, "%d.%m.%Y %H:%M:%S"))
                    except ValueError:
                        pass
            if date_objs:
                print(f"\n  –î–∞—Ç—ã –ø–æ—Å—Ç–æ–≤:")
                print(f"    –æ—Ç: {min(date_objs).strftime('%d.%m.%Y')}")
                print(f"    –¥–æ: {max(date_objs).strftime('%d.%m.%Y')}")

    # Show examples of filtered content
    rejected_examples = [p for p in posts_raw if is_junk_content(p["content"])]
    if rejected_examples:
        print(f"\n  –ü—Ä–∏–º–µ—Ä—ã –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö (–ø–µ—Ä–≤—ã–µ 3):")
        for p in rejected_examples[:3]:
            reason = is_junk_content(p["content"])
            preview = p["content"][:80].replace("\n", " ")
            print(f"    [{reason}] ¬´{preview}...¬ª")


# ‚îÄ‚îÄ‚îÄ Load into DB ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

async def load_posts(file_path: str, source: str, format: str = "html", dry_run: bool = False):
    """Load posts from a file into the database.

    Applies rule-based pre-filtering. Idempotent (checks content hashes).
    Use --dry-run to preview statistics without writing.
    """
    posts_raw = parse_file(file_path, format)
    posts_good, stats = filter_posts(posts_raw)

    if dry_run:
        print_dry_run_stats(source, file_path, posts_raw, posts_good, stats)
        return stats

    # Only import DB when actually writing
    _ensure_db_imports()

    print(f"–ù–∞–π–¥–µ–Ω–æ –ø–æ—Å—Ç–æ–≤: {len(posts_raw)}, –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(posts_good)} –∏–∑ {file_path}")

    async with get_session() as session:
        # Get existing content hashes for this source to avoid duplicates
        from sqlalchemy import select
        existing_result = await session.execute(
            select(KnowledgeBase.content).where(KnowledgeBase.source == source)
        )
        existing_hashes = {
            hashlib.md5(row[0].encode()).hexdigest()
            for row in existing_result.fetchall()
        }

        added = 0
        skipped_dup = 0
        for i, post in enumerate(posts_good):
            content_hash = hashlib.md5(post["content"].encode()).hexdigest()
            if content_hash in existing_hashes:
                skipped_dup += 1
                continue

            date = post.get("date")
            if isinstance(date, str):
                try:
                    date = datetime.strptime(date, "%d.%m.%Y %H:%M:%S")
                except (ValueError, TypeError):
                    date = None

            kb_entry = KnowledgeBase(
                source=source,
                content=post["content"],
                original_date=date,
                quality_score=0.5,  # Default ‚Äî will be scored by filter_quality.py
                is_active=True,
            )
            session.add(kb_entry)
            existing_hashes.add(content_hash)
            added += 1

            if added % 50 == 0 and added > 0:
                await session.flush()
                print(f"  –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {added}/{len(posts_good)}")

    print(
        f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {added} –Ω–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤ –∏–∑ {source} "
        f"(–¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {skipped_dup}, –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ: {stats['total_parsed'] - stats['accepted']})"
    )
    return stats


async def load_all(dry_run: bool = False):
    """Load all data sources."""
    data_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "data")

    total_stats = {"total_parsed": 0, "accepted": 0, "rejected": {}}

    def merge_stats(s):
        total_stats["total_parsed"] += s["total_parsed"]
        total_stats["accepted"] += s["accepted"]
        for k, v in s["rejected"].items():
            total_stats["rejected"][k] = total_stats["rejected"].get(k, 0) + v

    # –ù–∞—Å—Ç–∞–≤–Ω–∏—á–µ—Å—Ç–≤–æ channel (MD or HTML)
    # Try MD first (more common export), then HTML
    md_nastavnichestvo = os.path.join(data_dir, "–ª–æ–±–∞–Ω–æ–≤–æ.md")
    html_nastavnichestvo = os.path.join(data_dir, "–ª–æ–±–∞–Ω–æ–≤–æ.html")
    if os.path.exists(md_nastavnichestvo):
        s = await load_posts(md_nastavnichestvo, "nastavnichestvo_channel", "md", dry_run)
        if s:
            merge_stats(s)
    elif os.path.exists(html_nastavnichestvo):
        s = await load_posts(html_nastavnichestvo, "nastavnichestvo_channel", "html", dry_run)
        if s:
            merge_stats(s)
    else:
        print(f"‚ö†Ô∏è  –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {md_nastavnichestvo} –∏–ª–∏ {html_nastavnichestvo}")

    # Main channel (MD, 5 files)
    for i in range(1, 6):
        md_file = os.path.join(data_dir, f"–±–æ—Ä–æ–¥–∞—Ç{i}.md")
        if os.path.exists(md_file):
            s = await load_posts(md_file, "main_channel", "md", dry_run)
            if s:
                merge_stats(s)
        else:
            print(f"‚ö†Ô∏è  –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {md_file}")

    # Summary
    print(f"\n{'='*60}")
    print(f"üìä –ò–¢–û–ì–û –ø–æ –≤—Å–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º")
    print(f"{'='*60}")
    print(f"  –°–ø–∞—Ä—Å–µ–Ω–æ:       {total_stats['total_parsed']}")
    print(f"  –ü—Ä–∏–Ω—è—Ç–æ:        {total_stats['accepted']}")
    rejected_total = total_stats["total_parsed"] - total_stats["accepted"]
    print(f"  –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ:  {rejected_total}")
    if total_stats["rejected"]:
        for reason, count in sorted(total_stats["rejected"].items(), key=lambda x: -x[1]):
            print(f"    {reason}: {count}")

    if not dry_run:
        print(f"\n‚úÖ –í—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!")
        print(f"–°–ª–µ–¥—É—é—â–∏–π —à–∞–≥: python scripts/filter_quality.py")
    else:
        print(f"\nüí° –≠—Ç–æ dry-run. –î–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —É–±–µ—Ä–∏ --dry-run")


def main():
    parser = argparse.ArgumentParser(description="Load knowledge base from channel exports")
    parser.add_argument("--file", help="Path to export file")
    parser.add_argument("--source", help="Source identifier (nastavnichestvo_channel / main_channel)")
    parser.add_argument("--format", choices=["html", "md"], help="File format")
    parser.add_argument("--all", action="store_true", help="Load all files from data/")
    parser.add_argument("--dry-run", action="store_true", help="Parse and show statistics without writing to DB")

    args = parser.parse_args()

    if args.all:
        asyncio.run(load_all(dry_run=args.dry_run))
    elif args.file and args.source and args.format:
        asyncio.run(load_posts(args.file, args.source, args.format, dry_run=args.dry_run))
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
