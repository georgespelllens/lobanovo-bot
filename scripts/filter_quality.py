"""
LLM-based quality scoring for knowledge base posts.

Evaluates each post on a 0.0‚Äì1.0 scale:
  0.0‚Äì0.2  ‚Üí –º—É—Å–æ—Ä (—Ä–µ–ø–æ—Å—Ç—ã, –ø–æ–∑–¥—Ä–∞–≤–ª–µ–Ω–∏—è, –æ–ø—Ä–æ—Å—ã –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)
  0.3‚Äì0.5  ‚Üí —Å—Ä–µ–¥–Ω–µ (–æ–±—â–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –º–∞–ª–æ –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫–∏)
  0.6‚Äì0.8  ‚Üí —Ö–æ—Ä–æ—à–æ (–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Å–æ–≤–µ—Ç—ã, –∫–µ–π—Å—ã, –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏)
  0.9‚Äì1.0  ‚Üí –æ—Ç–ª–∏—á–Ω–æ (—É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–Ω—Å–∞–π—Ç –õ–æ–±–∞–Ω–æ–≤–∞, –∞–Ω—Ç–∏–ø–∞—Ç—Ç–µ—Ä–Ω—ã, –∫–µ–π—Å—ã —Å —Ü–∏—Ñ—Ä–∞–º–∏)

Posts with quality < threshold (default 0.3) are marked is_active=False.

Uses grok-4-1-fast-non-reasoning for speed and cost efficiency.
Estimated cost: ~$1.5‚Äì3 for 3000 posts.

Pipeline:
  1. load_knowledge_base.py --all          # –ó–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ—Å—Ç—ã
  2. filter_quality.py                     # LLM-–æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ ‚Üê –í–´ –ó–î–ï–°–¨
  3. filter_quality.py --dry-run           # –ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
  4. filter_quality.py --threshold 0.4     # –ü–æ–¥–Ω—è—Ç—å –ø–æ—Ä–æ–≥ (–∂—ë—Å—Ç—á–µ)
  5. generate_embeddings.py                # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ (—Ç–æ–ª—å–∫–æ active)

Usage:
  python scripts/filter_quality.py                           # –û—Ü–µ–Ω–∏—Ç—å –≤—Å–µ –Ω–µ–æ—Ü–µ–Ω—ë–Ω–Ω—ã–µ –ø–æ—Å—Ç—ã
  python scripts/filter_quality.py --dry-run                 # –ü–æ–∫–∞–∑–∞—Ç—å —Ç–µ–∫—É—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
  python scripts/filter_quality.py --threshold 0.4           # –ü–æ–¥–Ω—è—Ç—å –ø–æ—Ä–æ–≥ –æ—Ç—Å–µ—á–∫–∏
  python scripts/filter_quality.py --force                   # –ü–µ—Ä–µ–æ—Ü–µ–Ω–∏—Ç—å –≤—Å–µ –ø–æ—Å—Ç—ã
  python scripts/filter_quality.py --batch-size 20           # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
  python scripts/filter_quality.py --sample 10               # –û—Ü–µ–Ω–∏—Ç—å 10 —Å–ª—É—á–∞–π–Ω—ã—Ö (—Ç–µ—Å—Ç)
"""

import argparse
import asyncio
import json
import sys
import os

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from sqlalchemy import select, update, func, text
from src.database.connection import get_session, get_engine
from src.database.models import KnowledgeBase
from src.services.llm_service import call_llm
from src.utils.logger import logger


# ‚îÄ‚îÄ‚îÄ Quality scoring prompt ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

QUALITY_PROMPT = """–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∫–æ–Ω—Ç–µ–Ω—Ç-–º–∞—Ä–∫–µ—Ç–∏–Ω–≥—É –∏ –ª–∏—á–Ω–æ–º—É –±—Ä–µ–Ω–¥—É.
–û—Ü–µ–Ω–∏ –ø–æ—Å—Ç –∏–∑ Telegram-–∫–∞–Ω–∞–ª–∞ –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∏–Ω–∞ –õ–æ–±–∞–Ω–æ–≤–∞ –ø–æ —à–∫–∞–ª–µ –æ—Ç 0.0 –¥–æ 1.0.

–ö—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏:
- –ö–æ–Ω–∫—Ä–µ—Ç–∏–∫–∞: –µ—Å—Ç—å —Ü–∏—Ñ—Ä—ã, –ø—Ä–∏–º–µ—Ä—ã, –∫–µ–π—Å—ã? (+)
- –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å: –∞–≤—Ç–æ—Ä—Å–∫–æ–µ –º–Ω–µ–Ω–∏–µ –õ–æ–±–∞–Ω–æ–≤–∞, –∞ –Ω–µ –æ–±—â–∏–µ –∏—Å—Ç–∏–Ω—ã? (+)
- –ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å: —á–∏—Ç–∞—Ç–µ–ª—å –º–æ–∂–µ—Ç —á—Ç–æ-—Ç–æ —Å–¥–µ–ª–∞—Ç—å –ø–æ—Å–ª–µ –ø—Ä–æ—á—Ç–µ–Ω–∏—è? (+)
- –ì–ª—É–±–∏–Ω–∞: –µ—Å—Ç—å –∞–Ω–∞–ª–∏–∑, –∞–Ω—Ç–∏–ø–∞—Ç—Ç–µ—Ä–Ω—ã, –Ω–µ–æ—á–µ–≤–∏–¥–Ω—ã–µ –≤—ã–≤–æ–¥—ã? (+)
- –ú—É—Å–æ—Ä: —Ä–µ–ø–æ—Å—Ç —á—É–∂–æ–≥–æ, –ø–æ–∑–¥—Ä–∞–≤–ª–µ–Ω–∏–µ, —Ä–µ–∫–ª–∞–º–∞, –æ–ø—Ä–æ—Å –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞? (‚àí)
- –í–æ–¥–∞: –æ–±—â–∏–µ —Å–ª–æ–≤–∞ –±–µ–∑ –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫–∏, –º–æ—Ç–∏–≤–∞—Ü–∏—è –Ω–∏ –æ —á—ë–º? (‚àí)

–®–∫–∞–ª–∞:
  0.0‚Äì0.2 = –º—É—Å–æ—Ä (—Ä–µ–ø–æ—Å—Ç, –ø–æ–∑–¥—Ä–∞–≤–ª–µ–Ω–∏–µ, –æ–ø—Ä–æ—Å, —Ä–µ–∫–ª–∞–º–∞, —Å—Å—ã–ª–∫–∞ –±–µ–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è)
  0.3‚Äì0.5 = —Å—Ä–µ–¥–Ω–µ (–æ–±—â–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –º–∞–ª–æ –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫–∏, –Ω–æ –ø–æ —Ç–µ–º–µ)
  0.6‚Äì0.8 = —Ö–æ—Ä–æ—à–æ (–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Å–æ–≤–µ—Ç—ã, –ª–∏—á–Ω—ã–π –æ–ø—ã—Ç, –∫–µ–π—Å—ã)
  0.9‚Äì1.0 = –æ—Ç–ª–∏—á–Ω–æ (—É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–Ω—Å–∞–π—Ç, –∞–Ω—Ç–∏–ø–∞—Ç—Ç–µ—Ä–Ω, –∫–µ–π—Å —Å —Ü–∏—Ñ—Ä–∞–º–∏ –∏ –≤—ã–≤–æ–¥–∞–º–∏)

–û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
{"score": 0.7, "reason": "–∫—Ä–∞—Ç–∫–∞—è –ø—Ä–∏—á–∏–Ω–∞ –æ—Ü–µ–Ω–∫–∏ –≤ 5-10 —Å–ª–æ–≤"}"""


DEFAULT_THRESHOLD = 0.3
DEFAULT_BATCH_SIZE = 10


# ‚îÄ‚îÄ‚îÄ LLM scoring ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

async def score_post(content: str) -> tuple[float, str]:
    """Score a single post using LLM. Returns (score, reason)."""
    try:
        result = await call_llm(
            [
                {"role": "system", "content": QUALITY_PROMPT},
                {"role": "user", "content": content[:2000]},
            ],
            task_type="categorize",  # Uses fast model
            max_tokens=50,
            temperature=0.1,  # Low temp for consistent scoring
        )

        response = result["content"].strip()

        # Parse JSON response
        # Handle cases where LLM wraps in ```json
        if "```" in response:
            response = response.split("```")[1]
            if response.startswith("json"):
                response = response[4:]

        data = json.loads(response)
        score = float(data.get("score", 0.5))
        reason = data.get("reason", "")

        # Clamp to valid range
        score = max(0.0, min(1.0, score))
        return score, reason

    except json.JSONDecodeError:
        # Try to extract just a number
        import re
        numbers = re.findall(r"0\.\d+|1\.0|0|1", result["content"])
        if numbers:
            return float(numbers[0]), "json_parse_fallback"
        return 0.5, "parse_error"
    except Exception as e:
        logger.error(f"Score error: {e}")
        return 0.5, f"error: {type(e).__name__}"


# ‚îÄ‚îÄ‚îÄ Main operations ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

async def show_stats():
    """Show current quality distribution without changes."""
    async with get_session() as session:
        # Total posts
        total = await session.execute(select(func.count(KnowledgeBase.id)))
        total = total.scalar()

        # Active posts
        active = await session.execute(
            select(func.count(KnowledgeBase.id)).where(KnowledgeBase.is_active == True)
        )
        active = active.scalar()

        # Posts with quality_score != default (0.5)
        scored = await session.execute(
            select(func.count(KnowledgeBase.id)).where(
                KnowledgeBase.quality_score != 0.5
            )
        )
        scored = scored.scalar()

        # Quality distribution
        buckets_query = """
            SELECT
                CASE
                    WHEN quality_score < 0.2 THEN '0.0-0.2 (–º—É—Å–æ—Ä)'
                    WHEN quality_score < 0.4 THEN '0.2-0.4 (—Å–ª–∞–±–æ)'
                    WHEN quality_score < 0.6 THEN '0.4-0.6 (—Å—Ä–µ–¥–Ω–µ)'
                    WHEN quality_score < 0.8 THEN '0.6-0.8 (—Ö–æ—Ä–æ—à–æ)'
                    ELSE '0.8-1.0 (–æ—Ç–ª–∏—á–Ω–æ)'
                END as bucket,
                COUNT(*) as cnt
            FROM knowledge_base
            GROUP BY bucket
            ORDER BY bucket
        """
        result = await session.execute(text(buckets_query))
        distribution = result.fetchall()

        # Average quality
        avg = await session.execute(
            select(func.avg(KnowledgeBase.quality_score)).where(
                KnowledgeBase.quality_score != 0.5
            )
        )
        avg_score = avg.scalar()

    print(f"\n{'='*60}")
    print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π")
    print(f"{'='*60}")
    print(f"  –í—Å–µ–≥–æ –ø–æ—Å—Ç–æ–≤:      {total}")
    print(f"  –ê–∫—Ç–∏–≤–Ω—ã—Ö:          {active}")
    print(f"  –ù–µ–∞–∫—Ç–∏–≤–Ω—ã—Ö:        {total - active}")
    print(f"  –û—Ü–µ–Ω—ë–Ω–Ω—ã—Ö LLM:     {scored}")
    print(f"  –ù–µ –æ—Ü–µ–Ω—ë–Ω–Ω—ã—Ö:      {total - scored}")
    if avg_score:
        print(f"  –°—Ä–µ–¥–Ω–∏–π score:     {avg_score:.2f}")

    if distribution:
        print(f"\n  –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞:")
        for bucket, cnt in distribution:
            bar = "‚ñà" * (cnt // max(1, total // 40))
            print(f"    {bucket}: {cnt:>5} {bar}")


async def apply_threshold(threshold: float):
    """Mark posts below threshold as inactive."""
    async with get_session() as session:
        # Deactivate low-quality
        result = await session.execute(
            update(KnowledgeBase)
            .where(
                KnowledgeBase.quality_score < threshold,
                KnowledgeBase.quality_score != 0.5,  # Don't touch unscored
            )
            .values(is_active=False)
        )
        deactivated = result.rowcount

        # Reactivate above threshold (in case threshold was lowered)
        result2 = await session.execute(
            update(KnowledgeBase)
            .where(
                KnowledgeBase.quality_score >= threshold,
                KnowledgeBase.is_active == False,
            )
            .values(is_active=True)
        )
        reactivated = result2.rowcount

    print(f"  –ü–æ—Ä–æ–≥: {threshold}")
    print(f"  –î–µ–∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–æ (score < {threshold}): {deactivated}")
    print(f"  –†–µ–∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–æ (score >= {threshold}): {reactivated}")


async def score_posts(
    batch_size: int = DEFAULT_BATCH_SIZE,
    threshold: float = DEFAULT_THRESHOLD,
    force: bool = False,
    sample: int = 0,
    dry_run: bool = False,
):
    """Score posts using LLM and apply quality threshold."""

    if dry_run:
        await show_stats()
        return

    async with get_session() as session:
        # Get posts to score
        query = select(KnowledgeBase)
        if not force:
            # Only unscored (default 0.5)
            query = query.where(KnowledgeBase.quality_score == 0.5)

        if sample > 0:
            query = query.order_by(func.random()).limit(sample)

        result = await session.execute(query)
        posts = result.scalars().all()

        if not posts:
            print("–ù–µ—Ç –ø–æ—Å—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ (–≤—Å–µ —É–∂–µ –æ—Ü–µ–Ω–µ–Ω—ã). –ò—Å–ø–æ–ª—å–∑—É–π --force –¥–ª—è –ø–µ—Ä–µ–æ—Ü–µ–Ω–∫–∏.")
            await show_stats()
            return

        print(f"–ü–æ—Å—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏: {len(posts)}")
        total_cost = 0.0
        scored_count = 0

        for i, post in enumerate(posts):
            try:
                score, reason = await score_post(post.content)
                post.quality_score = score

                scored_count += 1

                if (i + 1) % batch_size == 0:
                    await session.flush()
                    print(
                        f"  [{i+1}/{len(posts)}] "
                        f"–ø–æ—Å–ª–µ–¥–Ω–∏–π: {score:.1f} ({reason[:30]})"
                    )

                # Rate limit
                await asyncio.sleep(0.15)

            except Exception as e:
                logger.error(f"Error scoring post #{post.id}: {e}")
                continue

        # Flush remaining
        await session.flush()

    print(f"\n‚úÖ –û—Ü–µ–Ω–µ–Ω–æ –ø–æ—Å—Ç–æ–≤: {scored_count}/{len(posts)}")

    # Apply threshold
    print(f"\n–ü—Ä–∏–º–µ–Ω—è—é –ø–æ—Ä–æ–≥ –∫–∞—á–µ—Å—Ç–≤–∞...")
    await apply_threshold(threshold)

    # Show final stats
    await show_stats()

    print(f"\n–°–ª–µ–¥—É—é—â–∏–π —à–∞–≥: python scripts/generate_embeddings.py")


def main():
    parser = argparse.ArgumentParser(
        description="LLM-based quality scoring for knowledge base",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã:
  %(prog)s                        –û—Ü–µ–Ω–∏—Ç—å –≤—Å–µ –Ω–µ–æ—Ü–µ–Ω—ë–Ω–Ω—ã–µ –ø–æ—Å—Ç—ã
  %(prog)s --dry-run              –ü–æ–∫–∞–∑–∞—Ç—å —Ç–µ–∫—É—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
  %(prog)s --threshold 0.4        –ü–æ–≤—ã—Å–∏—Ç—å –ø–æ—Ä–æ–≥ (–∂—ë—Å—Ç—á–µ)
  %(prog)s --force                –ü–µ—Ä–µ–æ—Ü–µ–Ω–∏—Ç—å –í–°–ï –ø–æ—Å—Ç—ã
  %(prog)s --sample 10            –¢–µ—Å—Ç –Ω–∞ 10 —Å–ª—É—á–∞–π–Ω—ã—Ö –ø–æ—Å—Ç–∞—Ö
        """,
    )
    parser.add_argument(
        "--batch-size", type=int, default=DEFAULT_BATCH_SIZE,
        help=f"Batch size for DB commits (default: {DEFAULT_BATCH_SIZE})"
    )
    parser.add_argument(
        "--threshold", type=float, default=DEFAULT_THRESHOLD,
        help=f"Quality threshold ‚Äî posts below this are deactivated (default: {DEFAULT_THRESHOLD})"
    )
    parser.add_argument(
        "--force", action="store_true",
        help="Re-score ALL posts (not just unscored)"
    )
    parser.add_argument(
        "--sample", type=int, default=0,
        help="Score only N random posts (for testing)"
    )
    parser.add_argument(
        "--dry-run", action="store_true",
        help="Show current stats without scoring"
    )

    args = parser.parse_args()

    asyncio.run(score_posts(
        batch_size=args.batch_size,
        threshold=args.threshold,
        force=args.force,
        sample=args.sample,
        dry_run=args.dry_run,
    ))


if __name__ == "__main__":
    main()
